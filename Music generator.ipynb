{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushar/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/tushar/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/tushar/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/tushar/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(file):\n",
    "    print(\"Loading music file\", file)\n",
    "    \n",
    "    notes = []\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    midi = converter.parse(file) #parsing midi files\n",
    "    \n",
    "    grp_ins = instrument.partitionByInstrument(midi) #partitioning based on different instrument\n",
    "    \n",
    "    for part in grp_ins.parts:\n",
    "        if 'Piano' in str(part):\n",
    "            notes_to_parse = part.recurse()\n",
    "            \n",
    "            # finding whether particular element is a note\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch)) #note\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder)) #chord\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading music file schubert/schub_d960_4.mid\n",
      "Loading music file schubert/schubert_D935_3.mid\n",
      "Loading music file schubert/schuim-1.mid\n",
      "Loading music file schubert/schubert_D850_2.mid\n",
      "Loading music file schubert/schub_d960_2.mid\n",
      "Loading music file schubert/schu_143_3.mid\n",
      "Loading music file schubert/schub_d760_1.mid\n",
      "Loading music file schubert/schub_d960_1.mid\n",
      "Loading music file schubert/schubert_D850_3.mid\n",
      "Loading music file schubert/schumm-4.mid\n",
      "Loading music file schubert/schubert_D935_1.mid\n",
      "Loading music file schubert/schumm-5.mid\n",
      "Loading music file schubert/schubert_D935_2.mid\n",
      "Loading music file schubert/schuim-2.mid\n",
      "Loading music file schubert/schuim-3.mid\n",
      "Loading music file schubert/schubert_D935_4.mid\n",
      "Loading music file schubert/schuim-4.mid\n",
      "Loading music file schubert/schumm-6.mid\n",
      "Loading music file schubert/schub_d960_3.mid\n",
      "Loading music file schubert/schubert_D850_1.mid\n",
      "Loading music file schubert/schubert_D850_4.mid\n",
      "Loading music file schubert/schu_143_2.mid\n",
      "Loading music file schubert/schub_d760_2.mid\n",
      "Loading music file schubert/schub_d760_4.mid\n",
      "Loading music file schubert/schumm-3.mid\n",
      "Loading music file schubert/schu_143_1.mid\n",
      "Loading music file schubert/schumm-1.mid\n",
      "Loading music file schubert/schumm-2.mid\n",
      "Loading music file schubert/schub_d760_3.mid\n"
     ]
    }
   ],
   "source": [
    "path = 'schubert/'\n",
    "files = [i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "#converting 2d array to 1d array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))\n",
    "\n",
    "freq = dict(Counter(notes_))\n",
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new array frequent music\n",
    "new_music = []\n",
    "for notes in notes_array:\n",
    "    temp = []\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)\n",
    "    new_music.append(temp)\n",
    "\n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing input and output sequences\n",
    "\n",
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning unique integer to every note\n",
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
    "\n",
    "#preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integer sequence for output\n",
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])\n",
    "\n",
    "# 80% training and rest 20% for evaluation\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 32, 100)           16700     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 167)               42919     \n",
      "=================================================================\n",
      "Total params: 267,939\n",
      "Trainable params: 267,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Dropout, MaxPool1D, Dense, Embedding, GlobalMaxPool1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "import keras.backend as k\n",
    "\n",
    "k.clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51530 samples, validate on 12883 samples\n",
      "Epoch 1/50\n",
      "51530/51530 [==============================] - 39s 766us/step - loss: 4.3404 - val_loss: 4.0236\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.02363, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "51530/51530 [==============================] - 37s 722us/step - loss: 3.8071 - val_loss: 3.7985\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.02363 to 3.79854, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "51530/51530 [==============================] - 39s 758us/step - loss: 3.6257 - val_loss: 3.6944\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.79854 to 3.69437, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      "51530/51530 [==============================] - 38s 744us/step - loss: 3.4924 - val_loss: 3.5952\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.69437 to 3.59523, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "51530/51530 [==============================] - 39s 748us/step - loss: 3.3921 - val_loss: 3.5247\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.59523 to 3.52475, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "51530/51530 [==============================] - 37s 727us/step - loss: 3.3067 - val_loss: 3.4365\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.52475 to 3.43645, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      "51530/51530 [==============================] - 36s 697us/step - loss: 3.2295 - val_loss: 3.4105\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.43645 to 3.41049, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      "51530/51530 [==============================] - 39s 758us/step - loss: 3.1645 - val_loss: 3.3254\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.41049 to 3.32545, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      "51530/51530 [==============================] - 37s 725us/step - loss: 3.1093 - val_loss: 3.2955\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.32545 to 3.29546, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      "51530/51530 [==============================] - 38s 736us/step - loss: 3.0561 - val_loss: 3.2938\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.29546 to 3.29382, saving model to best_model.h5\n",
      "Epoch 11/50\n",
      "51530/51530 [==============================] - 39s 761us/step - loss: 3.0064 - val_loss: 3.2310\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.29382 to 3.23102, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      "51530/51530 [==============================] - 38s 741us/step - loss: 2.9606 - val_loss: 3.1790\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.23102 to 3.17901, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      "51530/51530 [==============================] - 39s 749us/step - loss: 2.9202 - val_loss: 3.1591\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.17901 to 3.15912, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      "51530/51530 [==============================] - 38s 743us/step - loss: 2.8760 - val_loss: 3.1158\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.15912 to 3.11579, saving model to best_model.h5\n",
      "Epoch 15/50\n",
      "51530/51530 [==============================] - 39s 755us/step - loss: 2.8462 - val_loss: 3.1153\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.11579 to 3.11527, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      "51530/51530 [==============================] - 38s 731us/step - loss: 2.8099 - val_loss: 3.0765\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.11527 to 3.07650, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      "51530/51530 [==============================] - 38s 731us/step - loss: 2.7823 - val_loss: 3.0469\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.07650 to 3.04692, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      "51530/51530 [==============================] - 39s 755us/step - loss: 2.7471 - val_loss: 3.0286\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.04692 to 3.02863, saving model to best_model.h5\n",
      "Epoch 19/50\n",
      "51530/51530 [==============================] - 42s 813us/step - loss: 2.7229 - val_loss: 3.0121\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.02863 to 3.01213, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      "51530/51530 [==============================] - 39s 763us/step - loss: 2.6926 - val_loss: 3.0114\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.01213 to 3.01141, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      "51530/51530 [==============================] - 38s 741us/step - loss: 2.6755 - val_loss: 3.0061\n",
      "\n",
      "Epoch 00021: val_loss improved from 3.01141 to 3.00605, saving model to best_model.h5\n",
      "Epoch 22/50\n",
      "51530/51530 [==============================] - 41s 786us/step - loss: 2.6567 - val_loss: 2.9812\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.00605 to 2.98120, saving model to best_model.h5\n",
      "Epoch 23/50\n",
      "51530/51530 [==============================] - 43s 832us/step - loss: 2.6305 - val_loss: 2.9589\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.98120 to 2.95893, saving model to best_model.h5\n",
      "Epoch 24/50\n",
      "51530/51530 [==============================] - 38s 741us/step - loss: 2.6118 - val_loss: 2.9445\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.95893 to 2.94448, saving model to best_model.h5\n",
      "Epoch 25/50\n",
      "51530/51530 [==============================] - 39s 752us/step - loss: 2.5905 - val_loss: 2.9552\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/50\n",
      "51530/51530 [==============================] - 37s 724us/step - loss: 2.5712 - val_loss: 2.9255\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.94448 to 2.92550, saving model to best_model.h5\n",
      "Epoch 27/50\n",
      "51530/51530 [==============================] - 38s 732us/step - loss: 2.5611 - val_loss: 2.9209\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.92550 to 2.92091, saving model to best_model.h5\n",
      "Epoch 28/50\n",
      "51530/51530 [==============================] - 36s 693us/step - loss: 2.5373 - val_loss: 2.9045\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.92091 to 2.90448, saving model to best_model.h5\n",
      "Epoch 29/50\n",
      "51530/51530 [==============================] - 36s 702us/step - loss: 2.5287 - val_loss: 2.8877\n",
      "\n",
      "Epoch 00029: val_loss improved from 2.90448 to 2.88767, saving model to best_model.h5\n",
      "Epoch 30/50\n",
      "51530/51530 [==============================] - 36s 702us/step - loss: 2.5121 - val_loss: 2.8804\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.88767 to 2.88039, saving model to best_model.h5\n",
      "Epoch 31/50\n",
      "51530/51530 [==============================] - 36s 695us/step - loss: 2.4973 - val_loss: 2.8779\n",
      "\n",
      "Epoch 00031: val_loss improved from 2.88039 to 2.87787, saving model to best_model.h5\n",
      "Epoch 32/50\n",
      "51530/51530 [==============================] - 36s 703us/step - loss: 2.4894 - val_loss: 2.8679\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.87787 to 2.86793, saving model to best_model.h5\n",
      "Epoch 33/50\n",
      "51530/51530 [==============================] - 36s 698us/step - loss: 2.4719 - val_loss: 2.8553\n",
      "\n",
      "Epoch 00033: val_loss improved from 2.86793 to 2.85531, saving model to best_model.h5\n",
      "Epoch 34/50\n",
      "51530/51530 [==============================] - 36s 704us/step - loss: 2.4691 - val_loss: 2.8545\n",
      "\n",
      "Epoch 00034: val_loss improved from 2.85531 to 2.85452, saving model to best_model.h5\n",
      "Epoch 35/50\n",
      "51530/51530 [==============================] - 36s 697us/step - loss: 2.4518 - val_loss: 2.8440\n",
      "\n",
      "Epoch 00035: val_loss improved from 2.85452 to 2.84403, saving model to best_model.h5\n",
      "Epoch 36/50\n",
      "51530/51530 [==============================] - 36s 707us/step - loss: 2.4393 - val_loss: 2.8367\n",
      "\n",
      "Epoch 00036: val_loss improved from 2.84403 to 2.83668, saving model to best_model.h5\n",
      "Epoch 37/50\n",
      "51530/51530 [==============================] - 36s 696us/step - loss: 2.4351 - val_loss: 2.8338\n",
      "\n",
      "Epoch 00037: val_loss improved from 2.83668 to 2.83378, saving model to best_model.h5\n",
      "Epoch 38/50\n",
      "51530/51530 [==============================] - 36s 703us/step - loss: 2.4201 - val_loss: 2.8137\n",
      "\n",
      "Epoch 00038: val_loss improved from 2.83378 to 2.81373, saving model to best_model.h5\n",
      "Epoch 39/50\n",
      "51530/51530 [==============================] - 36s 693us/step - loss: 2.4143 - val_loss: 2.8027\n",
      "\n",
      "Epoch 00039: val_loss improved from 2.81373 to 2.80266, saving model to best_model.h5\n",
      "Epoch 40/50\n",
      "51530/51530 [==============================] - 36s 705us/step - loss: 2.3994 - val_loss: 2.8087\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/50\n",
      "51530/51530 [==============================] - 36s 697us/step - loss: 2.3925 - val_loss: 2.8143\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/50\n",
      "51530/51530 [==============================] - 36s 702us/step - loss: 2.3777 - val_loss: 2.8042\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/50\n",
      "51530/51530 [==============================] - 36s 702us/step - loss: 2.3716 - val_loss: 2.8000\n",
      "\n",
      "Epoch 00043: val_loss improved from 2.80266 to 2.79996, saving model to best_model.h5\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51530/51530 [==============================] - 37s 726us/step - loss: 2.3647 - val_loss: 2.7957\n",
      "\n",
      "Epoch 00044: val_loss improved from 2.79996 to 2.79572, saving model to best_model.h5\n",
      "Epoch 45/50\n",
      "51530/51530 [==============================] - 36s 698us/step - loss: 2.3544 - val_loss: 2.7912\n",
      "\n",
      "Epoch 00045: val_loss improved from 2.79572 to 2.79124, saving model to best_model.h5\n",
      "Epoch 46/50\n",
      "51530/51530 [==============================] - 36s 693us/step - loss: 2.3493 - val_loss: 2.7781\n",
      "\n",
      "Epoch 00046: val_loss improved from 2.79124 to 2.77813, saving model to best_model.h5\n",
      "Epoch 47/50\n",
      "51530/51530 [==============================] - 36s 704us/step - loss: 2.3482 - val_loss: 2.7879\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/50\n",
      "51530/51530 [==============================] - 36s 697us/step - loss: 2.3363 - val_loss: 2.7958\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/50\n",
      "51530/51530 [==============================] - 36s 708us/step - loss: 2.3298 - val_loss: 2.7778\n",
      "\n",
      "Epoch 00049: val_loss improved from 2.77813 to 2.77782, saving model to best_model.h5\n",
      "Epoch 50/50\n",
      "51530/51530 [==============================] - 36s 696us/step - loss: 2.3326 - val_loss: 2.7766\n",
      "\n",
      "Epoch 00050: val_loss improved from 2.77782 to 2.77664, saving model to best_model.h5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115, 50, 35, 149, 35, 115, 149, 35, 35, 35]\n"
     ]
    }
   ],
   "source": [
    "# predicted integer values\n",
    "import random\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions=[]\n",
    "for i in range(10):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting integer back into notes\n",
    "\n",
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the predictions to midi files\n",
    "def convert_to_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "            \n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
